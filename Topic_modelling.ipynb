{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    " \n",
    "data = []\n",
    " \n",
    "for fileid in gutenberg.fileids():\n",
    "    document = ' '.join(gutenberg.words(fileid))\n",
    "    data.append(document)\n",
    " \n",
    "NO_DOCUMENTS = len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(NO_DOCUMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim import models, corpora\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "NUM_TOPICS = 10\n",
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    cleaned_text = [t for t in tokenized_text if t not in STOPWORDS and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)]\n",
    "    stemmed = [porter.stem(word) for word in cleaned_text] \n",
    "    return stemmed\n",
    " \n",
    "# For gensim we need to tokenize the data and filter out stopwords\n",
    "tokenized_data = []\n",
    "for text in data:\n",
    "    tokenized_data.append(clean_text(text))\n",
    "\n",
    "\n",
    "   \n",
    " \n",
    "#Build a Dictionary - association word to numeric id\n",
    "dictionary = corpora.Dictionary(tokenized_data)\n",
    " \n",
    "# Transform the collection of texts to a numerical form\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n",
    " \n",
    "# Build the LDA model\n",
    "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
    " \n",
    "# Build the LSI model\n",
    "lsi_model = models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Model:\n",
      "Topic #0: 0.015*\"lord\" + 0.014*\"shall\" + 0.014*\"unto\" + 0.010*\"said\" + 0.007*\"one\" + 0.007*\"man\" + 0.007*\"thou\" + 0.006*\"thi\" + 0.006*\"god\" + 0.006*\"thee\"\n",
      "Topic #1: 0.012*\"shall\" + 0.011*\"said\" + 0.009*\"thou\" + 0.009*\"unto\" + 0.008*\"lord\" + 0.007*\"one\" + 0.006*\"thi\" + 0.006*\"man\" + 0.006*\"god\" + 0.006*\"day\"\n",
      "Topic #2: 0.015*\"shall\" + 0.013*\"lord\" + 0.013*\"unto\" + 0.009*\"said\" + 0.008*\"thou\" + 0.007*\"god\" + 0.007*\"thi\" + 0.007*\"thee\" + 0.006*\"day\" + 0.005*\"man\"\n",
      "Topic #3: 0.011*\"shall\" + 0.008*\"said\" + 0.008*\"lord\" + 0.008*\"unto\" + 0.006*\"thou\" + 0.006*\"god\" + 0.005*\"thee\" + 0.005*\"one\" + 0.005*\"man\" + 0.005*\"thing\"\n",
      "Topic #4: 0.008*\"said\" + 0.006*\"shall\" + 0.006*\"one\" + 0.006*\"man\" + 0.005*\"say\" + 0.005*\"thing\" + 0.004*\"thou\" + 0.004*\"would\" + 0.004*\"lord\" + 0.004*\"upon\"\n",
      "Topic #5: 0.010*\"shall\" + 0.009*\"said\" + 0.008*\"thou\" + 0.007*\"one\" + 0.006*\"man\" + 0.006*\"thi\" + 0.006*\"lord\" + 0.005*\"god\" + 0.005*\"upon\" + 0.005*\"thee\"\n",
      "Topic #6: 0.013*\"shall\" + 0.012*\"unto\" + 0.008*\"said\" + 0.008*\"lord\" + 0.007*\"thi\" + 0.006*\"god\" + 0.006*\"thou\" + 0.005*\"one\" + 0.005*\"say\" + 0.005*\"upon\"\n",
      "Topic #7: 0.011*\"said\" + 0.008*\"shall\" + 0.006*\"one\" + 0.005*\"say\" + 0.005*\"could\" + 0.005*\"day\" + 0.005*\"come\" + 0.004*\"upon\" + 0.004*\"lord\" + 0.004*\"unto\"\n",
      "Topic #8: 0.011*\"shall\" + 0.011*\"said\" + 0.007*\"lord\" + 0.007*\"unto\" + 0.006*\"thou\" + 0.005*\"one\" + 0.005*\"man\" + 0.004*\"would\" + 0.004*\"thee\" + 0.004*\"thi\"\n",
      "Topic #9: 0.009*\"shall\" + 0.008*\"unto\" + 0.007*\"one\" + 0.006*\"come\" + 0.006*\"lord\" + 0.005*\"man\" + 0.005*\"said\" + 0.005*\"god\" + 0.005*\"thou\" + 0.005*\"thi\"\n",
      "====================\n",
      "LSI Model:\n",
      "Topic #0: 0.415*\"shall\" + 0.377*\"unto\" + 0.336*\"lord\" + 0.231*\"thou\" + 0.199*\"god\" + 0.194*\"thi\" + 0.176*\"said\" + 0.162*\"thee\" + 0.147*\"son\" + 0.121*\"king\"\n",
      "Topic #1: 0.207*\"would\" + 0.197*\"one\" + 0.196*\"could\" + 0.193*\"said\" + -0.192*\"unto\" + -0.147*\"lord\" + 0.145*\"like\" + 0.143*\"whale\" + 0.134*\"look\" + 0.131*\"must\"\n",
      "Topic #2: -0.453*\"whale\" + 0.199*\"mr\" + -0.182*\"ship\" + 0.180*\"could\" + 0.164*\"said\" + -0.162*\"sea\" + -0.142*\"ahab\" + 0.142*\"emma\" + 0.138*\"miss\" + -0.136*\"boat\"\n",
      "Topic #3: -0.538*\"said\" + 0.228*\"emma\" + 0.156*\"whale\" + 0.150*\"mr\" + 0.145*\"everi\" + 0.133*\"harriet\" + 0.118*\"weston\" + -0.118*\"littl\" + 0.115*\"miss\" + 0.109*\"could\"\n",
      "Topic #4: 0.337*\"whale\" + -0.181*\"love\" + 0.171*\"unto\" + -0.168*\"earth\" + 0.162*\"said\" + -0.155*\"thee\" + -0.146*\"thi\" + 0.145*\"lord\" + -0.144*\"see\" + -0.141*\"heaven\"\n",
      "Topic #5: 0.400*\"elinor\" + 0.333*\"mariann\" + -0.322*\"emma\" + 0.188*\"sister\" + -0.188*\"harriet\" + 0.168*\"dashwood\" + -0.165*\"weston\" + 0.154*\"edward\" + -0.151*\"elton\" + -0.147*\"knightley\"\n",
      "Topic #6: 0.283*\"turnbul\" + 0.221*\"macian\" + 0.197*\"like\" + 0.193*\"man\" + 0.160*\"syme\" + -0.152*\"upon\" + 0.147*\"elinor\" + -0.135*\"good\" + 0.134*\"seem\" + -0.119*\"susan\"\n",
      "Topic #7: -0.307*\"heaven\" + -0.238*\"thu\" + -0.170*\"thou\" + -0.166*\"though\" + 0.160*\"see\" + -0.151*\"thi\" + 0.132*\"unto\" + -0.132*\"god\" + 0.125*\"soul\" + 0.125*\"everi\"\n",
      "Topic #8: 0.471*\"ann\" + 0.284*\"elliot\" + 0.278*\"captain\" + 0.210*\"wentworth\" + -0.182*\"elinor\" + 0.165*\"musgrov\" + 0.159*\"charl\" + -0.159*\"mariann\" + 0.145*\"ladi\" + 0.143*\"russel\"\n",
      "Topic #9: 0.584*\"littl\" + -0.253*\"turnbul\" + -0.198*\"macian\" + 0.136*\"came\" + 0.126*\"alic\" + 0.126*\"king\" + -0.109*\"upon\" + 0.105*\"could\" + -0.090*\"sir\" + -0.089*\"well\"\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "print(\"LDA Model:\")\n",
    " \n",
    "for idx in range(NUM_TOPICS):\n",
    "    # Print the first 10 most representative topics\n",
    "    print(\"Topic #%s:\" % idx, lda_model.print_topic(idx, 10))\n",
    " \n",
    "print(\"=\" * 20)\n",
    " \n",
    "print(\"LSI Model:\")\n",
    " \n",
    "for idx in range(NUM_TOPICS):\n",
    "    # Print the first 10 most representative topics\n",
    "    print(\"Topic #%s:\" % idx, lsi_model.print_topic(idx, 10))\n",
    " \n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.013056957753757379), (1, 0.0994094120198734), (2, 0.14846075268578937), (3, 0.22478003402701674), (4, -0.00028436995679137836), (5, -0.3273406658518878), (6, -0.042856612249100076), (7, 0.10299981018667717), (8, -0.1277115268464437), (9, 0.04509400138211044)]\n",
      "[(0, 0.025012754), (1, 0.02501381), (2, 0.025012048), (3, 0.025012705), (4, 0.025012277), (5, 0.025013205), (6, 0.025011422), (7, 0.025013603), (8, 0.7748856), (9, 0.025012609)]\n"
     ]
    }
   ],
   "source": [
    "text = \"Emma is rich and beautiful\"\n",
    "bow = dictionary.doc2bow(clean_text(text))\n",
    " \n",
    "print(lsi_model[bow])\n",
    "\n",
    " \n",
    "print(lda_model[bow])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
